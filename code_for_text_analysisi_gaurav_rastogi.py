# -*- coding: utf-8 -*-
"""Code for text analysisi-Gaurav Rastogi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cV5xDJrB3IsIuBlXXnolhyH00N-XbjS2
"""

// collecting stop words

my_file = open("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_Names.txt", "r")
  
# reading the file
data = my_file.read()
  
# replacing end splitting the text 
# when newline ('\n') is seen.
data_into_list = data.split("\n")
print(data_into_list)
my_file.close()

data_into_list.pop(0)

print(data_into_list)

data_into_list.insert(0,'SMITH')

my_file = open("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_Generic.txt", "r")
  
# reading the file
data = my_file.read()
  
# replacing end splitting the text 
# when newline ('\n') is seen.
data_into_list2 = data.split("\n")
print(data_into_list2)
my_file.close()

my_file = open("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_Auditor.txt", "r")
  
# reading the file
data = my_file.read()
  
# replacing end splitting the text 
# when newline ('\n') is seen.
data_into_list3 = data.split("\n")
print(data_into_list3)
my_file.close()

data_into_list3=data_into_list3[0:8]

print(data_into_list3)

my_file = open("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_Geographic.txt", "r")
  
# reading the file
data = my_file.read()
  
# replacing end splitting the text 
# when newline ('\n') is seen.
data_into_list4 = data.split("\n")
print(data_into_list4)
my_file.close()

data_into_list4.pop(0)

data_into_list4.remove('REPUBLIC  | Countries')
data_into_list4.remove('ALABAMA   | States')
data_into_list4.remove('YORK  | Cities')

data_into_list4.insert(0,'REPUBLIC')
data_into_list4.insert(20,'ALABAMA')
data_into_list4.insert(15,'YORK')
data_into_list4.insert(15,'UNITED')

file = open('/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_Currencies.txt', 'r', encoding="ISO-8859-1")
  
# reading the file
data = file.read()
  
# replacing end splitting the text 
# when newline ('\n') is seen.
data_into_list8 = data.split("\n")
print(data_into_list8)
my_file.close()

str(data_into_list8)

data_into=len(data_into_list8[0])

result_2 = [item.split('|') for item in data_into_list8]

print(result_2)

reult_ult=flat(result_2)

def flat(lis):
    flatList = []
    # Iterate with outer list
    for element in lis:
        if type(element) is list:
            # Check if type is list than iterate through the sublist
            for item in element:
                flatList.append(item)
        else:
            flatList.append(element)
    return flatList

print(reult_ult)

type(reult_ult[0])

print(data_into)

for rang

file = open('/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_Currencies.txt', 'r', encoding="ISO-8859-1")

my_file = open("/content/drive/MyDrive/20211030 Test Assignment/StopWords/StopWords_DatesandNumbers.txt", "r")
  
# reading the file
data = my_file.read()
  
# replacing end splitting the text 
# when newline ('\n') is seen.
data_into_list5 = data.split("\n")
print(data_into_list5)
my_file.close()

data_into_list5.remove('HUNDRED  | Denominations')
data_into_list5.remove('DATE  | Time related')
data_into_list5.remove('ONE  | Numbers')
data_into_list5.remove('I  | Roman numerals')

data_into_list5.insert(0,'HUNDRED')
data_into_list5.insert(20,'DATE')
data_into_list5.insert(15,'ONE')
data_into_list5.insert(30,'I')



combined_list = data_into_list + data_into_list2 + data_into_list3 + data_into_list4 + data_into_list5 + reult_ult

print(combined_list)

b=[]

for item in combined_list:
    b.append(item.lower())

for item in combined_list:
    b.append(item)

print(b)

print(b)

// merging our stop words to inbuilt

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
from nltk.tokenize import word_tokenize

nltk.download('punkt')

all_stopwords = stopwords.words('english')

all_stopwords.extend(b)

// collecting positive and negative words

print(all_stopwords)

my_file = open("/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary/positive-words.txt", "r")
  
# reading the file
data = my_file.read()
  
# replacing end splitting the text 
# when newline ('\n') is seen.
pos_list = data.split("\n")
print(pos_list)
my_file.close()

pos_listmain = [word for word in pos_list if not word in all_stopwords]

print(pos_listmain)

my_file = open("/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary/negative-words.txt",'r', encoding="ISO-8859-1")
  
# reading the file
data = my_file.read()
  
# replacing end splitting the text 
# when newline ('\n') is seen.
neg_list = data.split("\n")
print(neg_list)
my_file.close()

neg_list.remove('2-faced')
neg_list.remove('2-faces')

neg_listmain = [word for word in neg_list if not word in all_stopwords]

print(neg_listmain)

// links

import requests
import pandas as pd

# open the file
data = pd.read_excel('/content/drive/MyDrive/20211030 Test Assignment/Output Data Structure.xlsx')
# get the urls
urls = data.URL
print(urls)

print(urls)

urls=urls[0:114]

urls=list(urls)

print(len(urls))

type(urls)

print(urls[2])

import requests
from bs4 import BeautifulSoup as bs
import pickle
import pandas as pd 

# Scrapes transcript data from scrapsfromtheloft.com
def url_to_transcript(url):
    '''Returns transcript data specifically from scrapsfromtheloft.com.'''
    global data
    page=requests.get(url,headers={"User-Agent": "XY"})
    page.status_code
    soup = bs(page.content, 'html.parser')
    for data in soup.findAll('div', attrs={"class": "td-post-content"}):
        print(data.text)
    print(url)

    return data.text
    
    

#for loop to make keys of len(url)

transcripts = [url_to_transcript(u) for u in urls]

import shutil
shutil.rmtree('/content/transcripts', ignore_errors=False, onerror=None)

keys=[]

for i in range(0,114):
  a=str(i)
  keys.append(a)

print(keys)

!mkdir transcripts
#enumerate(keys)
for i, c in enumerate(keys):
     with open("transcripts/" + c + ".txt", "wb") as file:
         pickle.dump(transcripts[i], file)

data = {}
for i, c in enumerate(keys):
    with open("transcripts/" + c + ".txt", "rb") as file:
        data[c] = pickle.load(file)

!zip -r //content/transcripts.zip //content/transcriptstranscripts/Folder_To_Zip



#cross checking
data.keys()

next(iter(data.values()))

// to make list of all contents of all links and coverted into string

data_combined = {key: value for (key, value) in data.items()}

str(data_combined)

temp = data_combined.values()

print(temp)

tempnew=list(temp)

print(tempnew)

len(tempnew)

type(tempnew)

print(tempnew[113])

h=[]

// removing basic stop words not ours

newone=[]

from gensim.parsing.preprocessing import remove_stopwords

for i in range(0,114):
  n= remove_stopwords(tempnew[i])
  h.append(n)

print(h[0])

type(newone)

pip install textstat

pip install legacy_round

from google.colab import drive
drive.mount('/content/drive')
# copy it there
!cp transcripts  /content/drive/MyDrive

import string
import textstat
from textstat.textstat import textstatistics



// sentence calculation

import spacy
import re
def personalpro(text):
   c=0
   pronounRegex=re.compile(r'\b(I|we|my|ours|(?-i:us))\b',re.I)
   pronouns=pronounRegex.findall(text)
   c=len(pronouns)
   return c
def break_sentences(text):
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(text)
    return list(doc.sents)

def word_count():
    words = 0
    words = len(tokens_without_sw)
    return words

def syllables_count(word):
    return textstatistics().syllable_count(word)

def difficult_words():
    words = []
    #sentences = break_sentences(text)
    for sentence in sentences:
        words += [str(token) for token in tokens_without_sw]
 
    # difficult words are those with syllables >= 2
    # easy_word_set is provide by Textstat as
    # a list of common words
    diff_words_set = set()
     
    for word in words:
        syllable_count = syllables_count(word)
        if word not in all_stopwords and syllable_count >= 2:
            diff_words_set.add(word)
 
    return len(diff_words_set)

def sentence_count(text):
    sentences = break_sentences(text)
    return len(sentences)
 
def avg_sentence_length():
    words = word_count()
    e = len(sentences)
    average_sentence_length = float(words / e)
    return average_sentence_length

def avg_syllables_per_word(text):
    syllable = syllables_count(text)
    words = word_count()
    ASPW = float(syllable) / float(words)
    return ASPW

def percentage_of_complex():
   per_diff_words = (difficult_words() / word_count() )
   return per_diff_words

def gunning_fog():
    per_diff_words = percentage_of_complex()
    grade = 0.4 * (avg_sentence_length() + per_diff_words)
    return grade
def wordlength(text):
    count=0
    for i in range(0,len(text)):
      if text[i]!='':
        count=count+1
    return count
def positive_word_count():
    positivecount=0
    for word in tokens_without_sw:
       if word in pos_listmain:
          positivecount=positivecount+1
    return positivecount
def negative_word_count():
    countneg=0
    for word in tokens_without_sw:
       if word in neg_listmain:
           countneg=countneg+1
    return countneg

wordcount=[]
complexword=[]
noofsent=[]
fog_index=[]
average_sentence_len=[]
syllabeperword=[]
percentageofcomplex=[]
averagenoofword=[]
wordslength=[]
positivewords=[]
negativewords=[]
polarity=[]
subjectivity=[]
personalpronoun=[]

def addinto_list():
  wordcount.append(w)
  complexword.append(complexwords)
  noofsent.append(d)
  fog_index.append(fogindex)
  average_sentence_len.append(avgsenlength)
  syllabeperword.append( syllabesper_word)
  percentageofcomplex.append(percentageof_complex)
  averagenoofword.append(avgwords)
  wordslength.append(wordlen)
  positivewords.append(positive)
  negativewords.append(negative)
  polarity.append(PolarityScore)
  subjectivity.append(SubjectivityScore)
  personalpronoun.append(pp)

for i in range(0,114):
  sentences=break_sentences(h[i])
  d=len(sentences)
# d has sentence count
# cleaning  for tokens
  import re
  import string
  tempnew[i]= re.sub('[%s]' % re.escape(string.punctuation), '',tempnew[i])
  #newone[i] = newone.lower()
  tempnew[i] = re.sub('[‘’“”…]', '', tempnew[i])
  text_tokens = word_tokenize(tempnew[i])
  tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]
  w=word_count()
# w has word count
  complexwords=difficult_words()
# complex words has  complex words count
  fogindex= gunning_fog()
# fog index in fogindex
  avgsenlength=avg_sentence_length()
# average sentence length in avesenlength
  syllabesper_word=avg_syllables_per_word(tempnew[i])
# syllabe per word in syllabesper_word
  percentageof_complex = percentage_of_complex()
# pecentage of complex in percentageof?_complex
  avgwords= w/d
#average no of words in avgwords
  wordlen=wordlength(tempnew[i])
# words length in wordlen
  positive=positive_word_count()
  negative=negative_word_count()
  PolarityScore=(positive-negative)/((positive+negative) + 0.000001)
  SubjectivityScore = (positive+ negative)/(w+ 0.000001)
  pp=personalpro(tempnew[i])
  addinto_list()

print(wordslength)

import pandas as pd
import openpyxl
df = pd.DataFrame()

df['Positive score']=positivewords
df['Negative score']=negativewords
df['polarity']=polarity
df['subjectivity']=subjectivity
df['average sentence length']=average_sentence_len
df['percentage of complex']=percentageofcomplex
df['fog index']=fog_index
df['average number of words']= averagenoofword
df['complex words']=complexword
df['word count']=wordcount
df['syllabe per word']=syllabeperword
df['pesonal pronouns']=personalpronoun
df['words length']=wordslength
df['no of sentences']=noofsent

df.to_excel('resultupper and lower.xlsx', index = False)